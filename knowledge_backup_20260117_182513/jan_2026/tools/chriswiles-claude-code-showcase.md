---
title: "ChrisWiles/claude-code-showcase"
type: "tool"
date_added: "Wed Jan 14 23:21:09 +0000 2026"
source: "https://github.com/ChrisWiles/claude-code-showcase"
via: "Twitter bookmark from @brian_lovin"
url: "https://github.com/ChrisWiles/claude-code-showcase"
---

# ChrisWiles/claude-code-showcase

## Links

- [GitHub Repository](https://github.com/ChrisWiles/claude-code-showcase)
- [Original Tweet](https://x.com/brian_lovin/status/2011579369710657641)

## Original Tweet

> Idk, just type this into Claude Code and see what happens:

"""
Read the README on this repository: https://t.co/BjHHPqwrvw 

Explore the code base to learn about best practices and patterns for using Claude Code effectively. Take what you learn and bring it back into the context of our codebase. Look for opportunities to improve our own Claude Code setup (in the ./claude directory). Read our skills, commands, subagents, and rules files to get the full picture before making any suggestions.

When you're ready, create a list of your top recommendations and explain why you think they would be meaningful improvements to our project setup.

For each suggestion, create a testing framework so that you can measure how different prompts would behave or change the outcome given the new rule/setting/skill/etc.

The testing framework will let you run a sample prompt before making changes, take notes about the response/tool use/quality in your own response, and document everything in a BEFORE file. 

Then apply the suggested change to the rule/skill/command/setting file and re-run the exact same prompt. Observe the outcome and and save your notes in an AFTER file. 

Next, analyze the two documents to figure out if the changes to the Claude Code settings had a meaningful impact. If you aren't sure, run this loop with one more test prompt case (come up with the best possible prompt you can think of that would adequately test the changed setting).

Write a final before/after report that we can review together. When we find high impact changes, we'll implement the suggested change and commit. Don't stop until you've run the testing + evaluation loop on all of the recommendations from your first-phase exploration.
"""

â€” @brian_lovin (Brian Lovin)